\documentclass[12pt, article]{scrartcl}
\usepackage[english]{babel}
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}
\fancyfoot[L]{}
\fancyfoot[C]{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{13.6pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{	
\normalfont \normalsize 
\textsc{University of Idaho: CS 470 - Artificial Intelligence} \\ [25pt]
\horrule{0.5pt} \\[0.4cm]
\huge Project 1: Pathfinding\\
\horrule{2pt} \\[0.5cm]
}
\author{Andrew Schwartzmeyer}
\date{\normalsize\today}

\begin{document}
\maketitle % Print the title
\begin{abstract}
For this project I implemented a pathfinding program in Python to find the path from specified start to goal coordinates on a rectangular ASCII map. The agent is able to move up, down, left, and right, but not diagonally. I explored the efficiency of multiple common search algorithms used to solve this problem, including breadth-first, A* using Euclidean and Taxicab distance heuristics, uniform-cost, depth-first using a stack, recursive depth-first, and iterative deepening. \\
\end{abstract}
\pagebreak
\section{Introduction}
Pathfinding is a common problem in artificial intelligence. A map can be represented as a graph, with a specific start point as the root node, and another point as the goal node. With this graph representation of the map, graph traversal algorithms like the following can be used to find the shortest (or least cost) path. \\
\subsection{Maps}
The maps used in this project are rectangular grids of ASCII characters. We have: \\
\begin{center}
\begin{tabular}{l | l | l}
Character & Meaning & Movement cost \\ \hline
R & road & 1 \\
f & field & 2 \\
F & forest & 4 \\
h & hills & 5 \\
r & river & 7 \\
M & mountains & 10 \\
W & water & invalid moves \\
\end{tabular}
\end{center}

In the path and explored maps, `@' is the starting point, `*' is the path, `\$' is the goal, and `\#' is an explored state. \\ 

The sample map is of size 15 by 20, with a start coordinate of (7, 0) and goal coordinate of (7, 18). \\

\section{The Breadth-first Algorithm}
Breadth-first is an algorithm for small problems only, as it has a time and space complexity both of $O(b^{d})$. Essentially, it examines every possible path until it reaches the goal. It does this by visiting a node, then visiting all its neighbors, then for each of those neighbors, it visits their neighbors as well. Beacuse of this, it is both complete and optimal, in that it will find the solution (if one exists) and it will find the solution with the lowest cost, provided each move cost the same. \\

The algorithm is simple: 

\begin{verbatim}
create a fringe list
queue root node (start state) onto fringe list
while fringe list is not empty:
    remove front state from list
    if removed state is goal:
        then return path
    for neighbor of removed state:
        add neighbor to open list
\end{verbatim}

\begin{tabular}{p{2in} p{2in} p{2in}}
Map & Path & Explored \\
\begin{verbatim}
MMMhhffffffffff
MMMMMhhffffffff
hMMMhhhffFFFfff
fhMhffFFFFFFFff
fhhhffFFFFFFFFF
ffffFFFFFFFFfff
rrrrfFFFFFFffff
fffrrffFFFfffff
RRffrrrfFFFFfff
fRffffrFFFFFFff
fRfffWWWWWFFFFF
fRffWWWWWWWWFFF
fRRfffWWWWWWWrr
ffRRRRffffWWfff
fffffRRRfffffff
fffffffRfffffff
hffffffRRRRRRRR
Mhhffffffffffff
Mhhffffffffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
MMMhhff@fffffff
MMMMMhh*fffffff
hMMMhhh*fFFFfff
fhMhffF*FFFFFff
fhhhffF*FFFFFFF
ffffFFF*FFFFfff
rrrrfFF*FFFffff
fffrrff*FFfffff
RRffrrr*FFFFfff
fRff****FFFFFff
fRf**WWWWWFFFFF
fRf*WWWWWWWWFFF
fRR*ffWWWWWWWrr
ffR*RRffffWWfff
fff*fRRRfffffff
fff*fffRfffffff
hff*fffRRRRRRRR
Mhh*fffffffffff
Mhh****$fffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
#######@#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
####****#######
###**WWWWW#####
###*WWWWWWWW###
###*##WWWWWWW##
###*######WW###
###*###########
###*###########
###*###########
###*####ff#####
###****$fff####
#######fffff###
\end{verbatim}
\end{tabular}
\paragraph{Results}
From these results we can see the algorithm found the shortest possible path, but does not take into account cost. The space complexity of breadth-first is also demonstrated, with it exploring 267 nodes of 300, or 89 percent. It's path cost is an acceptable figure of 70. \\

\section{Depth-first search}
Depth-first search is the anti-thesis of breadth-first: instead of exploring neighbors before moving on, it explores as far as possible along each branch before zipping back and exploring the next. Its time complexity is $O(b^d)$, and its space complexity is $O(bd)$.
\begin{tabular}{p{2in} p{2in} p{2in}}
Map & Path & Explored \\

\begin{verbatim}
MMMhhffffffffff
MMMMMhhffffffff
hMMMhhhffFFFfff
fhMhffFFFFFFFff
fhhhffFFFFFFFFF
ffffFFFFFFFFfff
rrrrfFFFFFFffff
fffrrffFFFfffff
RRffrrrfFFFFfff
fRffffrFFFFFFff
fRfffWWWWWFFFFF
fRffWWWWWWWWFFF
fRRfffWWWWWWWrr
ffRRRRffffWWfff
fffffRRRfffffff
fffffffRfffffff
hffffffRRRRRRRR
Mhhffffffffffff
Mhhffffffffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
*******@fffffff
*MMMMhhffffffff
***************
fhMhffFFFFFFFf*
***************
*fffFFFFFFFFfff
***************
fffrrffFFFffff*
***************
*RffffrFFFFFFff
****#WWWWWFFFFF
fRf*WWWWWWWWFFF
****ffWWWWWWWrr
*fRRRRffffWWfff
***************
fffffffRffffff*
***************
*hhffffffffffff
*******$fffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
*******@fffffff
*MMMMhhffffffff
***************
fhMhffFFFFFFFf*
***************
*fffFFFFFFFFfff
***************
fffrrffFFFffff*
***************
*RffffrFFFFFFff
****fWWWWWFFFFF
fRf*WWWWWWWWFFF
****ffWWWWWWWrr
*fRRRRffffWWfff
***************
fffffffRffffff*
***************
*hhffffffffffff
*******$fffffff
MMhhhffffffffff
\end{verbatim}
\end{tabular}
\paragraph{Results}
Notice that depth-first is not optimal. I certainly would not want to take that path, considering its cost is 417; however, it was able to find that path by exploring only 123 nodes, or 41 percent. The space complexity of depth-first can be very useful. \\

\section{The A* Algorithm}
A* search is easily the most widely known search algorithm. It is an informed search strategy in that it uses problem-specific knowledge (other than the problem's definition), known as a heuristic. Informed searches are more efficient than uninformed ones. A* evaluates nodes using $f(n) = g(n) + h(n)$ where $g(n)$ is the action cost to the node $n$, and $h(n)$ is an estimate of the cheapest cost from $n$ to the goal. So when trying to find the least-cost solution, A* follows nodes of the lowest value of $f(n)$. This search is both complete and optimal, provided the heuristic $h(n)$ is \emph{admissible}, that is, it never overestimates the cost to reach the goal. \\

Here is the pseudocode of the guts of the algorithm:
\begin{verbatim}
while fringe list is not empty:
    remove state with lowest f(n)
    if removed state is goal:
        then return path
    for each neighbor of removed state:
        if neighbor not in fringe list OR new cost is lower than previous:
            add neighbor to openset
\end{verbatim}

\subsection{Euclidean Distance}
First we have A* using the Euclidean distance from $n$ to the goal for its heuristic. Euclidean distance is calculated as $sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$ where one point is the current node, and the other is the goal.\\

\begin{tabular}{p{2in} p{2in} p{2in}}
Map & Path & Explored \\

\begin{verbatim}
MMMhhffffffffff
MMMMMhhffffffff
hMMMhhhffFFFfff
fhMhffFFFFFFFff
fhhhffFFFFFFFFF
ffffFFFFFFFFfff
rrrrfFFFFFFffff
fffrrffFFFfffff
RRffrrrfFFFFfff
fRffffrFFFFFFff
fRfffWWWWWFFFFF
fRffWWWWWWWWFFF
fRRfffWWWWWWWrr
ffRRRRffffWWfff
fffffRRRfffffff
fffffffRfffffff
hffffffRRRRRRRR
Mhhffffffffffff
Mhhffffffffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
MMMhhff@fffffff
MMMMMhh*fffffff
hMMMhhh*fFFFfff
fhMhffF*FFFFFff
fhhhffF*FFFFFFF
ffffFFF*FFFFfff
rrrrfFF*FFFffff
fffrrff*FFfffff
RRffrrr*FFFFfff
fRff****FFFFFff
fRf**WWWWWFFFFF
fRf*WWWWWWWWFFF
fRR*ffWWWWWWWrr
ffR***ffffWWfff
fffff***fffffff
fffffff*fffffff
hffffff*RRRRRRR
Mhhffff*fffffff
Mhhffff$fffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
#######@#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
####****#######
###**WWWWW#####
###*WWWWWWWW###
###*##WWWWWWW#r
###***###fWWfff
#####***#ffffff
f######*#ffffff
h######*#RRRRRR
Mh#####*fffffff
Mhh###f$fffffff
MMhhhffffffffff
\end{verbatim}
\end{tabular}
\paragraph{Results}
With the Euclidean distance heuristic, A* found the path having explored 218 nodes (73 percent). Although this is not the cheapest possible path (like that found by uniform cost search) with a cost of 63, it is definitely acceptable. \\

\subsection{Taxicab Distance}
Secondly we have A* using the Taxicab distance from $n$ to the goal for its heuristic. Also known as the Manhattan distance, Taxicab geometry is the sum of the absolute differences between two coordinates, and is calculated as $|(x_1-x_2)| + |(y_1-y_2)|$ where one point is the current node, and the other is the goal.\\

\begin{tabular}{p{2in} p{2in} p{2in}}
Map & Path & Explored \\

\begin{verbatim}
MMMhhffffffffff
MMMMMhhffffffff
hMMMhhhffFFFfff
fhMhffFFFFFFFff
fhhhffFFFFFFFFF
ffffFFFFFFFFfff
rrrrfFFFFFFffff
fffrrffFFFfffff
RRffrrrfFFFFfff
fRffffrFFFFFFff
fRfffWWWWWFFFFF
fRffWWWWWWWWFFF
fRRfffWWWWWWWrr
ffRRRRffffWWfff
fffffRRRfffffff
fffffffRfffffff
hffffffRRRRRRRR
Mhhffffffffffff
Mhhffffffffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
MMMhhff@fffffff
MMMMMhh*fffffff
hMMMhhh*fFFFfff
fhMhffF*FFFFFff
fhhhffF*FFFFFFF
ffffFFF*FFFFfff
rrrrfFF*FFFffff
fffrrff*FFfffff
RRffrrr*FFFFfff
fRff****FFFFFff
fRf**WWWWWFFFFF
fRf*WWWWWWWWFFF
fRR*ffWWWWWWWrr
ffR***ffffWWfff
fffff***fffffff
fffffff*fffffff
hffffff*RRRRRRR
Mhhffff*fffffff
Mhhffff$fffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
#######@#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
#######*#######
####****#######
###**WWWWW#####
###*WWWWWWWW###
###*##WWWWWWW#r
###***###fWWfff
#####***#ffffff
f######*#ffffff
h######*#RRRRRR
Mh#####*fffffff
Mhh###f$fffffff
MMhhhffffffffff
\end{verbatim}
\end{tabular}
\paragraph{Results}
The results are almost the same as with the Euclidean heuristic. With the Taxicab distance heuristic, A* found the path having explored 217 nodes (72 percent), and with a path cost of 63 as well. On more complex maps this is a greater difference, but these tests are not shown for brevity (though their results will be discussed). \\

\section{Uniform-cost Algorithm}
Uniform-cost search is actually a special implementation of the A* algorithm, and it is also both complete and optimal. In fact, in the code, uniform-cost search simply calls the A* subroutine with a ``monotonic heuristic.'' In the cost calculation formula $f(n) = g(n) + h(n)$, $h(n)$ is zero for uniform-cost search, hence its name. It does, however, still take $g(n)$ (specific movement costs) into account. Interestingly, the breadth-first algorithm is itself a special case of uniform-cost, where instead of taking into account a varied movement cost, all costs are the same, and thus it searches instead for the shortest path. \\

\begin{tabular}{p{2in} p{2in} p{2in}}
Map & Path & Explored \\

\begin{verbatim}
MMMhhffffffffff
MMMMMhhffffffff
hMMMhhhffFFFfff
fhMhffFFFFFFFff
fhhhffFFFFFFFFF
ffffFFFFFFFFfff
rrrrfFFFFFFffff
fffrrffFFFfffff
RRffrrrfFFFFfff
fRffffrFFFFFFff
fRfffWWWWWFFFFF
fRffWWWWWWWWFFF
fRRfffWWWWWWWrr
ffRRRRffffWWfff
fffffRRRfffffff
fffffffRfffffff
hffffffRRRRRRRR
Mhhffffffffffff
Mhhffffffffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
MMMhhff@fffffff
MMMMMhh*fffffff
hMMMhhh*fFFFfff
fhMh****FFFFFff
fhhh*fFFFFFFFFF
ff***FFFFFFFfff
rr*rfFFFFFFffff
ff*rrffFFFfffff
R**frrrfFFFFfff
f*ffffrFFFFFFff
f*fffWWWWWFFFFF
f*ffWWWWWWWWFFF
f**fffWWWWWWWrr
ff****ffffWWfff
fffff***fffffff
fffffff*fffffff
hffffff*RRRRRRR
Mhhffff*fffffff
Mhhffff$fffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
#######@#######
#######*#######
#######*#######
####****#######
####*##########
##***##########
##*############
##*############
#**############
#*#############
#*###WWWWW#####
#*##WWWWWWWW###
#**###WWWWWWW##
##****####WW###
#####***#######
#######*##ff###
#######*###R###
M######*#ffffff
Mhh###f$fffffff
MMhhhffffffffff
\end{verbatim}
\end{tabular}
\paragraph{Results}
Uniform cost search provides us with the cheapest possible path cost of 58, having explored 241 nodes, or 80 percent, which is unfortunately close to that of breadth first. \\

\section{Iterative Deepening Algorithm}
Iterative deepening depth-first search is an interesting and tricky to implement heuristic. The strategy is to repeatedly run a depth-limited search, increasing the depth with each iteration until the goal is found. It is equivalent to a breadth-first search but uses far less memory. Each iteration visits nodes in the same order as a depth-first search, but the cumulative order is akin to a breadth-first search. Because of this, it has both the space-efficiency of depth-first, and completeness and optimality of breadth-first. Its space complexity is $O(bd)$, and its time complexity is $O(b^d)$. \\

This particular implementation runs a cost-limited search. Instead of limiting the depth of the search, it limits the cost of the path, it expands the nodes in the order of increasing path cost, and iteratively increases the cost limit until the solution is found. This is also known as an iterative lengthening search, but is not commonly used because of its substantial overhead, and because it does not find the truly lowest cost path (such as that given by uniform-cost search). However, its space complexity and ability to find a relatively cheap path is very useful. \\ \\
\begin{tabular}{p{2in} p{2in} p{2in}}
Map & Path & Explored \\

\begin{verbatim}
MMMhhffffffffff
MMMMMhhffffffff
hMMMhhhffFFFfff
fhMhffFFFFFFFff
fhhhffFFFFFFFFF
ffffFFFFFFFFfff
rrrrfFFFFFFffff
fffrrffFFFfffff
RRffrrrfFFFFfff
fRffffrFFFFFFff
fRfffWWWWWFFFFF
fRffWWWWWWWWFFF
fRRfffWWWWWWWrr
ffRRRRffffWWfff
fffffRRRfffffff
fffffffRfffffff
hffffffRRRRRRRR
Mhhffffffffffff
Mhhffffffffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
MMMhhff@fffffff
MMMMMhh*fffffff
hMMMhhh**FFFfff
fhMhffFF*FFFFff
fhhhffFF*FFFFFF
ffffFFFF*FFFfff
rrrrfFFF*FFffff
fffrrffF*Ffffff
RRffrrr**FFFfff
fRff****FFFFFff
fRf**WWWWWFFFFF
fRf*WWWWWWWWFFF
fRR*ffWWWWWWWrr
ffR***ffffWWfff
fffff***fffffff
fffffff*fffffff
hffffff*RRRRRRR
Mhhffff*fffffff
Mhhffff$fffffff
MMhhhffffffffff
\end{verbatim}
&
\begin{verbatim}
MMMhhff@fffffff
MMMMMhh*fffffff
hMMMhhh**FFFfff
fhMhffFF*FFFFff
fhhhffFF*FFFFFF
ffffFFFF*FFFfff
rrrrfFFF*FFffff
fffrrffF*Ffffff
RRffrrr**FFFfff
fRff****FFFFFff
fRf**WWWWWFFFFF
fRf*WWWWWWWWFFF
fRR*ffWWWWWWWrr
ffR***ffffWWfff
fffff***fffffff
fffffff*fffffff
hffffff*RRRRRRR
Mhhffff*fffffff
Mhhffff$fffffff
MMhhhffffffffff
\end{verbatim}
\end{tabular}
\paragraph{Results}
Iterative deepening depth-limited (while ignoring repeated states) found a path of cost 69, which while not the cheapest, is incredibly acceptable considering it explored only 35 nodes (12 percent). However, keep in mind that this was the final number of explored nodes, and that it had to repeatedly run the search close to 70 times to acheieve that. Fortunately, these repeated searches aren't too costly as the previous paths started at cost 0, increasing each iteration until the final path was reached. \\

\section{Cumulative Results}

%\begin{tabular}{p{2in} p{2in} p{2in}}
%Map & Path & Explored \\

%\begin{verbatim}
%\end{verbatim}
%&
%\begin{verbatim}
%\end{verbatim}
%&
%\begin{verbatim}
%\end{verbatim}
%\end{tabular}
\end{document}
